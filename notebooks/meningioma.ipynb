{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/project\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import SimpleITK\n",
    "\n",
    "os.chdir('/opt/project')\n",
    "print(os.getcwd())"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-11T15:11:48.274695300Z",
     "start_time": "2023-08-11T15:11:48.171961900Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import warnings\n",
    "import pandas as pd\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "# for copying meningioma data from the original folder structure to one suitable with autorad\n",
    "\n",
    "from multiprocessing import Pool\n",
    "\n",
    "\n",
    "def process_patient(patient_info):\n",
    "    patient_src, dest_patient_folder = patient_info\n",
    "    expected_files = {'ADC.nii.gz', 'flair.nii.gz', 't1.nii.gz', 't1ce.nii.gz', 't2.nii.gz', 'mask.nii.gz'}\n",
    "    found_files = set()\n",
    "    for file in os.listdir(patient_src):\n",
    "        if file.endswith('.nii') or file.endswith('.gz'):\n",
    "            if 'ADC' in file:\n",
    "                new_filename = 'ADC.nii'\n",
    "            elif 'flair' in file:\n",
    "                new_filename = 'flair.nii.gz'\n",
    "            elif 't1ce' in file:\n",
    "                new_filename = 't1ce.nii.gz'\n",
    "            elif 't1' in file:\n",
    "                new_filename = 't1.nii.gz'\n",
    "            elif 't2' in file:\n",
    "                new_filename = 't2.nii.gz'\n",
    "            elif '_1.nii' in file or '_2.nii' in file or '_3.nii' in file:\n",
    "                new_filename = 'mask.nii.gz'\n",
    "            else:\n",
    "                warnings.warn(f\"Unexpected file {file} in patient folder {patient_src}\")\n",
    "                continue\n",
    "\n",
    "            src_file_path = os.path.join(patient_src, file)\n",
    "            dest_file_path = os.path.join(dest_patient_folder, new_filename)\n",
    "            shutil.copy(src_file_path, dest_file_path)\n",
    "            print(f\"Copied file {src_file_path} to {dest_file_path}\")\n",
    "\n",
    "            found_files.add(new_filename)\n",
    "\n",
    "    if expected_files != found_files:\n",
    "        missing_files = expected_files - found_files\n",
    "        warnings.warn(f\"Missing files {missing_files} in patient folder {patient_src}\")\n",
    "\n",
    "    return dest_patient_folder\n",
    "\n",
    "\n",
    "def copy_patient_data(src_dir, dest_dir, n_cpu=2):\n",
    "    \"\"\"\n",
    "    Copies patient data from the source directory to the destination directory\n",
    "    while maintaining an ascending pattern of integer-based patient IDs. The function\n",
    "    creates a mapping of patient IDs, grades, and their original data paths,\n",
    "    and saves the mapping to a CSV file. It also renames the files as per the required format.\n",
    "\n",
    "    Parameters:\n",
    "        src_dir (str): The path to the source directory containing patient data organized by grades.\n",
    "        dest_dir (str): The path to the destination directory where the copied patient data will be stored.\n",
    "\n",
    "    Returns:\n",
    "        None: The function performs the copy operation, file renaming, and saves the mapping to a CSV file.\n",
    "\n",
    "    Raises:\n",
    "        FileNotFoundError: If any of the expected files are missing.\n",
    "        ValueError: If there are additional files that don't match the pattern.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(dest_dir):\n",
    "        os.mkdir(dest_dir)\n",
    "    patient_data = []\n",
    "    patient_info_list = []\n",
    "    for grade_folder in os.listdir(src_dir):\n",
    "        grade_path = os.path.join(src_dir, grade_folder)\n",
    "        if os.path.isdir(grade_path):\n",
    "            for patient_folder in os.listdir(grade_path):\n",
    "                patient_src = os.path.join(grade_path, patient_folder)\n",
    "                if os.path.isdir(patient_src):\n",
    "                    dest_patient_id = len(os.listdir(dest_dir)) + 1\n",
    "                    dest_patient_folder = os.path.join(dest_dir, f\"ID_{dest_patient_id}\")\n",
    "                    os.makedirs(dest_patient_folder, exist_ok=True)\n",
    "                    patient_info_list.append((patient_src, dest_patient_folder))\n",
    "                    grade = 1 if grade_folder == 'Grade1Data' else 2\n",
    "                    patient_data.append((f\"ID_{dest_patient_id}\", grade, patient_src))\n",
    "\n",
    "    # Process patient data in parallel using multiprocessing\n",
    "    with Pool(n_cpu) as pool:\n",
    "        dest_patient_folders = pool.map(process_patient, patient_info_list)\n",
    "\n",
    "    # Create a pandas DataFrame from the patient_data list\n",
    "    df = pd.DataFrame(patient_data, columns=[\"Patient_ID\", \"Grade\", \"Original_Data_Path\"])\n",
    "\n",
    "    # Save the DataFrame to a CSV file\n",
    "    df.to_csv(\"patient_grade_mapping.csv\", index=False)\n",
    "\n",
    "\n",
    "copy_patient_data('./data/meningioma', './data/meningioma_data')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-06T11:56:03.286817900Z",
     "start_time": "2023-08-06T11:55:54.576730700Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 640x480 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure()\n",
    "plt.imshow(sitk.GetArrayFromImage(image)[12,:,:])\n",
    "plt.imshow(sitk.GetArrayFromImage(mask)[12,:,:], alpha=0.7)\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-06T12:00:28.864927800Z",
     "start_time": "2023-08-06T12:00:28.755567800Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([360, 1, 8, 8])"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "import torch\n",
    "\n",
    "data = load_digits(n_class=2)\n",
    "\n",
    "images, target = data['images'], data['target']\n",
    "\n",
    "images = torch.unsqueeze(torch.tensor(images), dim=1)\n",
    "\n",
    "images.shape"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-11T15:11:55.118698800Z",
     "start_time": "2023-08-11T15:11:53.701301Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "No y-values are given (y=None). You must either supply a Dataset as X or implement your own DataLoader for training (and your validation) and supply it using the ``iterator_train`` and ``iterator_valid`` parameters respectively.",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[3], line 24\u001B[0m\n\u001B[1;32m      3\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01msrc\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mmodels\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mencoder\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Encoder\n\u001B[1;32m      5\u001B[0m encoder \u001B[38;5;241m=\u001B[39m Encoder(VanillaVAE,\n\u001B[1;32m      6\u001B[0m                   module__in_channels\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m,\n\u001B[1;32m      7\u001B[0m                   module__latent_dim\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m100\u001B[39m,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     21\u001B[0m                   \u001B[38;5;66;03m#   ))]\u001B[39;00m\n\u001B[1;32m     22\u001B[0m                   )\n\u001B[0;32m---> 24\u001B[0m \u001B[43mencoder\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mimages\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/opt/project/src/models/encoder.py:32\u001B[0m, in \u001B[0;36mEncoder.fit\u001B[0;34m(self, X, y)\u001B[0m\n\u001B[1;32m     31\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mfit\u001B[39m(\u001B[38;5;28mself\u001B[39m, X, y\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m):\n\u001B[0;32m---> 32\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[1;32m     33\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\n",
      "File \u001B[0;32m/opt/conda/lib/python3.10/site-packages/skorch/classifier.py:143\u001B[0m, in \u001B[0;36mNeuralNetClassifier.fit\u001B[0;34m(self, X, y, **fit_params)\u001B[0m\n\u001B[1;32m    132\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"See ``NeuralNet.fit``.\u001B[39;00m\n\u001B[1;32m    133\u001B[0m \n\u001B[1;32m    134\u001B[0m \u001B[38;5;124;03mIn contrast to ``NeuralNet.fit``, ``y`` is non-optional to\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    138\u001B[0m \n\u001B[1;32m    139\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    140\u001B[0m \u001B[38;5;66;03m# pylint: disable=useless-super-delegation\u001B[39;00m\n\u001B[1;32m    141\u001B[0m \u001B[38;5;66;03m# this is actually a pylint bug:\u001B[39;00m\n\u001B[1;32m    142\u001B[0m \u001B[38;5;66;03m# https://github.com/PyCQA/pylint/issues/1085\u001B[39;00m\n\u001B[0;32m--> 143\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mNeuralNetClassifier\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mfit_params\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/opt/conda/lib/python3.10/site-packages/skorch/net.py:1302\u001B[0m, in \u001B[0;36mNeuralNet.fit\u001B[0;34m(self, X, y, **fit_params)\u001B[0m\n\u001B[1;32m   1299\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mwarm_start \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39minitialized_:\n\u001B[1;32m   1300\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39minitialize()\n\u001B[0;32m-> 1302\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpartial_fit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mfit_params\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1303\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\n",
      "File \u001B[0;32m/opt/conda/lib/python3.10/site-packages/skorch/net.py:1261\u001B[0m, in \u001B[0;36mNeuralNet.partial_fit\u001B[0;34m(self, X, y, classes, **fit_params)\u001B[0m\n\u001B[1;32m   1259\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnotify(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mon_train_begin\u001B[39m\u001B[38;5;124m'\u001B[39m, X\u001B[38;5;241m=\u001B[39mX, y\u001B[38;5;241m=\u001B[39my)\n\u001B[1;32m   1260\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m-> 1261\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit_loop\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mfit_params\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1262\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mKeyboardInterrupt\u001B[39;00m:\n\u001B[1;32m   1263\u001B[0m     \u001B[38;5;28;01mpass\u001B[39;00m\n",
      "File \u001B[0;32m/opt/conda/lib/python3.10/site-packages/skorch/net.py:1155\u001B[0m, in \u001B[0;36mNeuralNet.fit_loop\u001B[0;34m(self, X, y, epochs, **fit_params)\u001B[0m\n\u001B[1;32m   1119\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mfit_loop\u001B[39m(\u001B[38;5;28mself\u001B[39m, X, y\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, epochs\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mfit_params):\n\u001B[1;32m   1120\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"The proper fit loop.\u001B[39;00m\n\u001B[1;32m   1121\u001B[0m \n\u001B[1;32m   1122\u001B[0m \u001B[38;5;124;03m    Contains the logic of what actually happens during the fit\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   1153\u001B[0m \n\u001B[1;32m   1154\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m-> 1155\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcheck_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1156\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcheck_training_readiness()\n\u001B[1;32m   1157\u001B[0m     epochs \u001B[38;5;241m=\u001B[39m epochs \u001B[38;5;28;01mif\u001B[39;00m epochs \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmax_epochs\n",
      "File \u001B[0;32m/opt/conda/lib/python3.10/site-packages/skorch/classifier.py:116\u001B[0m, in \u001B[0;36mNeuralNetClassifier.check_data\u001B[0;34m(self, X, y)\u001B[0m\n\u001B[1;32m    106\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m (\n\u001B[1;32m    107\u001B[0m         (y \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m) \u001B[38;5;129;01mand\u001B[39;00m\n\u001B[1;32m    108\u001B[0m         (\u001B[38;5;129;01mnot\u001B[39;00m is_dataset(X)) \u001B[38;5;129;01mand\u001B[39;00m\n\u001B[1;32m    109\u001B[0m         (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39miterator_train \u001B[38;5;129;01mis\u001B[39;00m DataLoader)\n\u001B[1;32m    110\u001B[0m ):\n\u001B[1;32m    111\u001B[0m     msg \u001B[38;5;241m=\u001B[39m (\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mNo y-values are given (y=None). You must either supply a \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    112\u001B[0m            \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDataset as X or implement your own DataLoader for \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    113\u001B[0m            \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtraining (and your validation) and supply it using the \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    114\u001B[0m            \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m``iterator_train`` and ``iterator_valid`` parameters \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    115\u001B[0m            \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrespectively.\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m--> 116\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(msg)\n\u001B[1;32m    117\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m y \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    118\u001B[0m     \u001B[38;5;66;03m# pylint: disable=attribute-defined-outside-init\u001B[39;00m\n\u001B[1;32m    119\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mclasses_inferred_ \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39munique(to_numpy(y))\n",
      "\u001B[0;31mValueError\u001B[0m: No y-values are given (y=None). You must either supply a Dataset as X or implement your own DataLoader for training (and your validation) and supply it using the ``iterator_train`` and ``iterator_valid`` parameters respectively."
     ]
    }
   ],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from src.dataset.dl_dataset import SitkImageProcessor\n",
    "from src.utils.prepro_utils import get_multi_paths_with_separate_folder_per_case\n",
    "from src.models.autoencoder import VanillaVAE, Encoder, VAELoss\n",
    "\n",
    "\n",
    "sitk_processor = SitkImageProcessor('./outputs', './data/meningioma_data', mask_stem='mask',\n",
    "                                    image_stems=('registered_adc', 't2', 'flair', 't1', 't1ce'), n_jobs=6)\n",
    "\n",
    "encoder = Encoder(VanillaVAE,\n",
    "                  module__in_channels=5,\n",
    "                  module__latent_dim=100,\n",
    "                  module__hidden_dims= [16, 32, 64],\n",
    "                  module__finish_size=2,\n",
    "                  criterion=VAELoss,\n",
    "                  std_dim=(0,2,3,4),\n",
    "                  max_epochs=10,\n",
    "                  output_format='pandas'\n",
    "                  )\n",
    "\n",
    "autoencoder_pipeline = Pipeline(steps=[\n",
    "    (\"read_data\", sitk_processor),\n",
    "    ('encoder', encoder)\n",
    "])\n",
    "\n",
    "pipeline = ColumnTransformer(transformers=[('autoencoder', autoencoder_pipeline, 'ID')], remainder='passthrough', verbose_feature_names_out=False)\n",
    "pipeline.set_output(transform='pandas')\n",
    "pipeline.fit(feature_dataset.data.X.train_folds[0])\n",
    "fitted_val = pipeline.transform(feature_dataset.data.X.val_folds[0])\n",
    "print(fitted_val)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-11T15:11:58.899974Z",
     "start_time": "2023-08-11T15:11:56.735841100Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "output_dir = './outputs/meningioma/2023-09-01-01-32-46'\n",
    "# dataset = get_data('./data/meningioma_data', 't1ce', 'mask')\n",
    "# sitk_images = get_sitk_images(dataset, n_jobs=5)\n",
    "\n",
    "# setup dataset, extract features, split the data\n",
    "feature_dataset = get_multimodal_feature_dataset(data_dir='./data/meningioma_data',\n",
    "                                                 image_stems=('registered_adc', 't2', 'flair', 't1', 't1ce'),\n",
    "                                                 mask_stem='mask',\n",
    "                                                 target_column='Grade',\n",
    "                                                 label_csv_path='./data/meningioma_meta.csv',\n",
    "                                                 extraction_params='./conf/radiomic_params/meningioma_mr.yaml',\n",
    "                                                 feature_df_merger={'_target_': 'src.pipeline.df_mergers.meningioma_df_merger'},\n",
    "                                                 n_jobs=6,\n",
    "                                                 existing_feature_df= os.path.join(output_dir, 'extracted_features.csv'),\n",
    "                                                 additional_features=['ID']\n",
    "                                                 )\n",
    "\n",
    "# feature_dataset.df.to_csv(\"./outputs/meningioma_feature_dataset.csv\")\n",
    "\n",
    "feature_dataset = split_feature_dataset(feature_dataset,\n",
    "                                        existing_split=os.path.join(output_dir,'splits.yml'))\n",
    "\n",
    "# models = MLClassifier.initialize_default_sklearn_models()\n",
    "\n",
    "# run auto preprocessing\n",
    "# run_auto_preprocessing(data=feature_dataset.data,\n",
    "#                        result_dir=Path(output_dir),\n",
    "#                        use_oversampling= False,\n",
    "#                        feature_selection_methods=['anova', 'lasso'],\n",
    "#                        use_feature_selection=True,\n",
    "#                        autoencoder={'_target_': 'sklearn.pipeline.make_pipeline',\n",
    "#                                     '_args_':[\n",
    "#                                         {'_target_': 'src.dataset.dl_dataset.SitkImageProcessor',\n",
    "#                                          'result_dir': './outputs',\n",
    "#                                          'data_dir': './data/meningioma_data',\n",
    "#                                          'image_stems': ('registered_adc', 't2', 'flair', 't1', 't1ce'),\n",
    "#                                          'mask_stem': 'mask',\n",
    "#                                          'n_jobs': 6},\n",
    "#                                         {'_target_': 'src.models.autoencoder.nn_encoder.Encoder',\n",
    "#                                          'module': 'vanillavae',\n",
    "#                                          'module__in_channels': 5,\n",
    "#                                          'module__latent_dim': 256,\n",
    "#                                          'module__hidden_dims':[16,32, 64],\n",
    "#                                          'module__finish_size': 2,\n",
    "#                                          'std_dim':[0, 2, 3, 4],\n",
    "#                                          'max_epochs': 10,\n",
    "#                                          'output_format': 'pandas'}]}\n",
    "# )\n",
    "sitk_processor = SitkImageProcessor('./outputs', './data/meningioma_data', mask_stem='mask',\n",
    "                                    image_stems=('registered_adc', 't2', 'flair', 't1', 't1ce'), n_jobs=6)\n",
    "\n",
    "encoder = Encoder(ResnetVAE,\n",
    "                  module__in_channels=5,\n",
    "                  module__latent_dim=128,\n",
    "                  module__hidden_dims=[32, 64, 128],\n",
    "                  module__finish_size=2,\n",
    "                  criterion=MSSIM,\n",
    "                  std_dim=(0, 2, 3, 4),\n",
    "                  max_epochs=200,\n",
    "                  output_format='pandas',\n",
    "                  callbacks=[EarlyStopping(load_best=True),\n",
    "                             GradientNormClipping(1),\n",
    "                             # SimpleLoadInitState(f_optimizer='outputs/saved_models/optimizer.pt',\n",
    "                             #                     f_params='outputs/saved_models/params.pt')\n",
    "                             ],\n",
    "                  optimizer=torch.optim.AdamW,\n",
    "                  lr=0.001,\n",
    "                  # criterion__loss_type='B',\n",
    "                  # criterion__gamma=10.0,\n",
    "                  # criterion__max_capacity=1,\n",
    "\n",
    "                  # criterion__alpha=10.0,\n",
    "                  # criterion__beta=1.0\n",
    "                  criterion__in_channels=5,\n",
    "                  criterion__window_size=4,\n",
    "                  criterion__kld_weight=0.001,\n",
    "                  transform_kwargs=dict(thetaX=(-90, 90),\n",
    "                                        thetaY=(-90, 90),\n",
    "                                        thetaZ=(-90, 90),\n",
    "                                        tx=(-1, 1),\n",
    "                                        ty=(-1, 1),\n",
    "                                        tz=(-1, 1),\n",
    "                                        scale=(1, 1),\n",
    "                                        n=10),\n",
    "                  device='cuda'\n",
    "                  )\n",
    "\n",
    "# param_grid = {\n",
    "#     \"module__latent_dim\": [128, 256, 512, 1024],\n",
    "#     'lr': [0.01, 0.001, 0.0001],\n",
    "#     # 'criterion__beta': [],\n",
    "#     'criterion__gamma': [10, 100, 1000, 10000],\n",
    "#     'criterion__kld_weight': [0.001, 0.01, 0.0001],\n",
    "#     'criterion__max_capacity': [0,5,10,20,40]\n",
    "#\n",
    "# }\n",
    "#\n",
    "# trainer = EncoderTrainer(encoder, param_grid, feature_dataset,sitk_processor)\n",
    "# trainer.run(wandb_kwargs={'project': 'autoencoder_tuning',\n",
    "#                           'dir': './outputs',\n",
    "#                           # 'mode': 'off'\n",
    "#                           })\n",
    "\n",
    "for i, (train_x, train_y, val_x, val_y) in enumerate(zip(feature_dataset.data.X.train_folds, feature_dataset.data.y.train_folds, feature_dataset.data.X.val_folds, feature_dataset.data.y.val_folds)):\n",
    "    # _encoder = type(encoder)(**encoder.get_params())\n",
    "    _encoder = clone(encoder)\n",
    "\n",
    "    images = sitk_processor.fit_transform(train_x['ID'])\n",
    "    _encoder.fit(images)\n",
    "\n",
    "    generated_images = _encoder.generate(images)\n",
    "\n",
    "    plot_slices(generated_images, 8, 2, original_tensor=dfsitk2tensor(images), title=datetime.now().strftime(f\"%Y%m%d%H%M%S-fold{i}\"))\n",
    "\n",
    "    # break"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def plot_debug(stk_image):\n",
    "    plt.figure()\n",
    "    plt.imshow(sitk.GetArrayFromImage(stk_image)[5, :, :], cmap='gray')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_slices(output_tensor, slice_index, num_samples=5, original_tensor=None,\n",
    "                title=None, save_dir=None):\n",
    "    \"\"\"\n",
    "    Plot a slice from each image modality of the output tensor for a specified number of samples.\n",
    "\n",
    "    Parameters:\n",
    "        output_tensor (torch.Tensor): The output tensor from the autoencoder.\n",
    "        slice_index (int): The index of the slice to be plotted.\n",
    "        image_modalities (list): List of image modality names.\n",
    "        num_samples (int): The number of samples to plot.\n",
    "        title_prefix (str): Prefix to add to the plot titles.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    batch_size, num_modalities, length, width, height = output_tensor.shape\n",
    "\n",
    "    for sample_idx in range(min(num_samples, batch_size)):\n",
    "        plt.figure(figsize=(15, 5))  # Adjust the figure size as needed\n",
    "\n",
    "        for modality_idx in range(num_modalities):\n",
    "            plt.subplot(2, num_modalities, modality_idx + 1)\n",
    "            plt.imshow(output_tensor[sample_idx, modality_idx, slice_index, :, :], cmap='gray')\n",
    "            plt.title(f'generated Sample {sample_idx + 1}, {modality_idx}')\n",
    "            plt.axis('off')\n",
    "\n",
    "        if original_tensor is not None:\n",
    "            for modality_idx in range(num_modalities):\n",
    "                plt.subplot(2, num_modalities, num_modalities + modality_idx + 1)\n",
    "                plt.imshow(original_tensor[sample_idx, modality_idx, slice_index, :, :], cmap='gray')\n",
    "                plt.title(f'original Sample {sample_idx + 1}, {modality_idx}')\n",
    "                plt.axis('off')\n",
    "\n",
    "        if title is not None:\n",
    "            plt.suptitle(title)\n",
    "        if save_dir is not None:\n",
    "            plt.savefig(f'{save_dir}_{sample_idx}.png')\n",
    "\n",
    "        plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from torchio import SubjectsDataset\n",
    "from src.dataset import TransformingDataLoader, SkorchSubjectsDataset\n",
    "from skorch.callbacks import EarlyStopping, GradientNormClipping\n",
    "from src.models.autoencoder import SegResNetVAE2, BetaVAELoss\n",
    "from src.pipeline.pipeline_components import get_multimodal_feature_dataset, split_feature_dataset\n",
    "import torchio as tio\n",
    "from datetime import datetime\n",
    "\n",
    "# test bench\n",
    "output_dir = './outputs/meningioma/2023-09-01-01-32-46'\n",
    "# dataset = get_data('./data/meningioma_data', 't1ce', 'mask')\n",
    "# sitk_images = get_sitk_images(dataset, n_jobs=5)\n",
    "\n",
    "# setup dataset, extract features, split the data\n",
    "feature_dataset = get_multimodal_feature_dataset(data_dir='./data/meningioma_data',\n",
    "                                                 image_stems=('registered_adc', 't2', 'flair', 't1', 't1ce'),\n",
    "                                                 mask_stem='mask',\n",
    "                                                 target_column='Grade',\n",
    "                                                 label_csv_path='./data/meningioma_meta.csv',\n",
    "                                                 extraction_params='./conf/radiomic_params/meningioma_mr.yaml',\n",
    "                                                 feature_df_merger={\n",
    "                                                     '_target_': 'src.pipeline.df_mergers.meningioma_df_merger'},\n",
    "                                                 n_jobs=6,\n",
    "                                                 existing_feature_df=os.path.join(output_dir, 'extracted_features.csv'),\n",
    "                                                 additional_features=['ID']\n",
    "                                                 )\n",
    "\n",
    "# feature_dataset.df.to_csv(\"./outputs/meningioma_feature_dataset.csv\")\n",
    "\n",
    "feature_dataset = split_feature_dataset(feature_dataset,\n",
    "                                        existing_split=os.path.join(output_dir, 'splits.yml'))\n",
    "\n",
    "sitk_processor = SitkImageProcessor('./data/meningioma_data', mask_stem='mask',\n",
    "                                    image_stems=('registered_adc', 't2', 'flair', 't1', 't1ce'))\n",
    "\n",
    "subject_list = sitk_processor.subject_list\n",
    "\n",
    "encoder = Encoder(SegResNetVAE2,\n",
    "                    module__input_image_size=[64, 64, 64],\n",
    "                    module__spatial_dims=3,\n",
    "                    module__in_channels=5,\n",
    "                    module__out_channels=5,\n",
    "                    module__dropout_prob=0.2,\n",
    "                    module__init_filters=32,\n",
    "                    # output_format='pandas',\n",
    "                    criterion=BetaVAELoss,\n",
    "                    max_epochs=200,\n",
    "                    callbacks=[EarlyStopping(load_best=True),\n",
    "                               GradientNormClipping(1),\n",
    "\n",
    "                               # SimpleLoadInitState(f_optimizer='outputs/saved_models/optimizer.pt',\n",
    "                               #                     f_params='outputs/saved_models/params.pt')\n",
    "                               ],\n",
    "                    optimizer=torch.optim.AdamW,\n",
    "                    batch_size=4,\n",
    "                    lr=0.001,\n",
    "                    iterator_train=TransformingDataLoader,\n",
    "                    iterator_train__augment_transforms=tio.Compose([tio.RandomGamma(log_gamma=0.1, label_keys='mask'),\n",
    "                                                                    tio.RandomAffine(p=0.5, label_keys='mask',\n",
    "                                                                                     scales=0.1, degrees=0,\n",
    "                                                                                     translation=0, isotropic=True),\n",
    "                                                                    tio.RandomFlip(flip_probability=0.5,\n",
    "                                                                                   label_keys='mask', axes=(0, 1, 2))\n",
    "                                                                    ]),\n",
    "                    iterator_train__num_workers=6,\n",
    "                    iterator_train__shuffle=True,\n",
    "                    iterator_valid__num_workers=6,\n",
    "                    dataset=SkorchSubjectsDataset,\n",
    "                    dataset__transform=tio.Compose([tio.Resample((1, 1, 1)),\n",
    "                                                    tio.ToCanonical(),\n",
    "                                                    tio.Mask(masking_method='mask', outside_value=0),\n",
    "                                                    tio.CropOrPad(target_shape=(64, 64, 64), mask_name='mask'),\n",
    "                                                    tio.ZNormalization(masking_method='mask')]),\n",
    "                    # criterion__gamma=10.0,\n",
    "                    # criterion__max_capacity=1,\n",
    "\n",
    "                    # criterion__alpha=10.0,\n",
    "                    # criterion__beta=1.0\n",
    "                    # criterion__in_channels=5,\n",
    "                    # criterion__window_size=4,\n",
    "                    criterion__kld_weight=0.1,\n",
    "                    # criterion__finish_size=8,\n",
    "                    device='cuda'\n",
    "                    )\n",
    "\n",
    "encoder.fit(subject_list)\n",
    "\n",
    "generated_images = encoder.predict(subject_list[:10])\n",
    "\n",
    "subject_dataset = SubjectsDataset(subject_list[:10], transform=tio.Compose([tio.Resample((1, 1, 1)),\n",
    "                                                    tio.ToCanonical(),\n",
    "                                                    tio.Mask(masking_method='mask', outside_value=0),\n",
    "                                                    tio.CropOrPad(target_shape=(64, 64, 64), mask_name='mask'),\n",
    "                                                    tio.ZNormalization(masking_method='mask')]))\n",
    "\n",
    "original_images = torch.stack(\n",
    "    [torch.concatenate([i.data for i in subject.get_images()]) for subject in subject_dataset])\n",
    "plot_slices(generated_images, original_tensor=original_images, slice_index=32, num_samples=8, title=datetime.now().strftime(f\"%Y%m%d%H%M%S\"),\n",
    "            save_dir=f'outputs/generated_images/{datetime.now().strftime(f\"%Y%m%d%H%M%S\")}')"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
