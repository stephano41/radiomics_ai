{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Meningioma Experiment Replication\n",
    "\n",
    "Start by cloning this repository and with reference to the README at the root of the repository, build the docker image and setup the jupyter notebook server to connect to this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/project\n"
     ]
    }
   ],
   "source": [
    "# Ensure we are in the right directory\n",
    "import os\n",
    "\n",
    "os.chdir('/opt/project')\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation\n",
    "<a id='data-preparation'></a>\n",
    "To replicate the experiment, you'll need T1, T2, T1 contrast enhanced (T1CE), Fluid-attenuated inversion recovery (FLAIR), and apparent diffusion coefficient (ADC) imaging in a `nii.gz` format. Additionally, a 3D segmentation mask in `.nii.gz` format is required.\n",
    "\n",
    "First, preprocess the T1, T2, T1CE, and FLAIR images using the [Cancer Imaging Phenomics Toolkit with the BraTS Pre-processing Pipeline](https://cbica.github.io/CaPTk/preprocessing_brats.html) with no skull stripping or deep learning segmentation enabled. Assume your preprocessed T1, T2, T1CE, FLAIR, and unregistered ADC appear as follows in your directory:\n",
    "\n",
    "radiomica_ai/\\\n",
    "│\\\n",
    "└── data/\\\n",
    "&emsp;│\\\n",
    "&emsp;└── meningioma_data/\\\n",
    "&emsp;&emsp;│\\\n",
    "&emsp;&emsp;└── ID_00001/\\\n",
    "&emsp;&emsp;&emsp;├── t1.nii.gz\\\n",
    "&emsp;&emsp;&emsp;├── t2.nii.gz\\\n",
    "&emsp;&emsp;&emsp;├── t1ce.nii.gz\\\n",
    "&emsp;&emsp;&emsp;├── flair.nii.gz\\\n",
    "&emsp;&emsp;&emsp;├── unregistered_adc.nii.gz\\\n",
    "&emsp;&emsp;&emsp;└── mask.nii.gz\n",
    "\n",
    "Then, register the ADC imaging to T1CE using rigid transformation as below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.preprocessing import register_patients\n",
    "\n",
    "register_patients(data_dir='./data/meningioma_data',\n",
    "                  static_stem='t1ce.nii.gz',\n",
    "                  moving_stem='unregistered_adc.nii.gz', \n",
    "                  output_stem='registered_adc.nii.gz', \n",
    "                  transform_method='rigid',\n",
    "                  id_regex=\"ID_*\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replicating the pipeline\n",
    "**Handcrafted radiomics only model**\n",
    "\n",
    "`docker compose run app python main.py experiment=meningioma optimizer.n_trials=1000`\n",
    "\n",
    "From which the best feature selection method was mrmr and best oversampling method was SMOTETomek\n",
    "\n",
    "`docker compose run app python main.py experiment=meningioma optimizer.n_trials=500 split.method=repeated_stratified_kfold_no_test +split.n_repeats=20 \"preprocessing.feature_selection_methods=[mrmr]\" \"preprocessing.oversampling_methods=[SMOTETomek]\"`\n",
    "\n",
    "**Handcrafted + deep learning radiomics model**\n",
    "\n",
    "First download the [pretrained FMCIB model weights](https://zenodo.org/records/10528450/files/model_weights.torch?download=1), store it in outputs/pretrained_models/model_weights.torch, otherwise specify the path in preprocessing.autoencoder.module__weights_path argument\n",
    "\n",
    "`docker compose run app python main.py experiment=meningioma_autoencoder optimizer.n_trials=1000`\n",
    "\n",
    "From which the best feature selection method was lasso and best oversampling method was SMOTEENN\n",
    "\n",
    "`docker compose run app python main.py experiment=meningioma_autoencoder optimizer.n_trials=500 split.method=repeated_stratified_kfold_no_test +split.n_repeats=20 \"preprocessing.feature_selection_methods=[lasso]\" \"preprocessing.oversampling_methods=[SMOTEENN]\"`\n",
    "\n",
    "The results of the run can be seen in the mlflow dashboard by launching the mlflow server. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference on new data\n",
    "To infer on new data using the pretrained pipeline, you'll need the zip file of the mlflow run folder. Download and extract the folder, paste the folder in `outputs/models/`. Then retrieve the run ID from mlflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HRO_run_id = 'af832aa5f6a74696b3b0e4f9c5034ffd'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The test directory is expected to have the same directory structure, and have already undergone the same steps in [Data Preparation](#data-preparation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.inference import Inferrer\n",
    "\n",
    "inferrer = Inferrer(HRO_run_id,\n",
    "                    image_stems=('registered_adc', 't2', 'flair', 't1', 't1ce'),\n",
    "                    mask_stem='mask',\n",
    "                    extraction_config='./conf/radiomic_params/meningioma_mr.yaml')\n",
    "\n",
    "inferrer.predict('data/test_meningioma_data')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
